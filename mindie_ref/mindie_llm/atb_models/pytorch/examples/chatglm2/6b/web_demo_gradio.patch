---
 web_demo.py | 91 ++++++++++++++++++++++++++++++++---------------------
 1 file changed, 56 insertions(+), 35 deletions(-)

diff --git a/web_demo.py b/web_demo.py
index 1af24c9..3d2e158 100644
--- a/web_demo.py
+++ b/web_demo.py
@@ -1,14 +1,9 @@
-from transformers import AutoModel, AutoTokenizer
 import gradio as gr
 import mdtex2html
-from utils import load_model_on_gpus
+import torch
+import torch_npu
 
-tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
-model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True).cuda()
-# 多显卡支持，使用下面两行代替上面一行，将num_gpus改为你实际的显卡数量
-# from utils import load_model_on_gpus
-# model = load_model_on_gpus("THUDM/chatglm2-6b", num_gpus=2)
-model = model.eval()
+from main import get_model, override_topp_and_topk, parse_args
 
 """Override Chatbot.postprocess"""
 
@@ -62,6 +57,12 @@ def parse_text(text):
 
 def predict(input, chatbot, max_length, top_p, temperature, history, past_key_values):
     chatbot.append((parse_text(input), ""))
+    # 因为点击事件会令起线程执行，所以在主线程 set_device 对点击事件里的 stream_chat 无效
+    # 需要在点击事件里 stream_chat 执行前 set_device，否则会出现 context 空指针报错
+    torch.npu.set_device(args.device)
+    if args.tp_size > 1:
+        object_list = [input, max_length, top_p, temperature]
+        torch.distributed.broadcast_object_list(object_list, src=0)
     for response, history, past_key_values in model.stream_chat(tokenizer, input, history, past_key_values=past_key_values,
                                                                 return_past_key_values=True,
                                                                 max_length=max_length, top_p=top_p,
@@ -79,30 +80,50 @@ def reset_state():
     return [], [], None
 
 
-with gr.Blocks() as demo:
-    gr.HTML("""<h1 align="center">ChatGLM2-6B</h1>""")
-
-    chatbot = gr.Chatbot()
-    with gr.Row():
-        with gr.Column(scale=4):
-            with gr.Column(scale=12):
-                user_input = gr.Textbox(show_label=False, placeholder="Input...", lines=10).style(
-                    container=False)
-            with gr.Column(min_width=32, scale=1):
-                submitBtn = gr.Button("Submit", variant="primary")
-        with gr.Column(scale=1):
-            emptyBtn = gr.Button("Clear History")
-            max_length = gr.Slider(0, 32768, value=8192, step=1.0, label="Maximum length", interactive=True)
-            top_p = gr.Slider(0, 1, value=0.8, step=0.01, label="Top P", interactive=True)
-            temperature = gr.Slider(0, 1, value=0.95, step=0.01, label="Temperature", interactive=True)
-
-    history = gr.State([])
-    past_key_values = gr.State(None)
-
-    submitBtn.click(predict, [user_input, chatbot, max_length, top_p, temperature, history, past_key_values],
-                    [chatbot, history, past_key_values], show_progress=True)
-    submitBtn.click(reset_user_input, [], [user_input])
-
-    emptyBtn.click(reset_state, outputs=[chatbot, history, past_key_values], show_progress=True)
-
-demo.queue().launch(share=False, inbrowser=True)
+override_topp_and_topk()
+args = parse_args()
+tokenizer, model = get_model(args)
+
+is_rank_0 = (args.tp_size == 1) or (torch.distributed.get_rank() == 0)
+if is_rank_0:
+    with gr.Blocks() as demo:
+        gr.HTML("""<h1 align="center">ChatGLM2-6B</h1>""")
+
+        chatbot = gr.Chatbot()
+        with gr.Row():
+            with gr.Column(scale=4):
+                with gr.Column(scale=12):
+                    user_input = gr.Textbox(show_label=False, placeholder="Input...", lines=10).style(
+                        container=False)
+                with gr.Column(min_width=32, scale=1):
+                    submitBtn = gr.Button("Submit", variant="primary")
+            with gr.Column(scale=1):
+                emptyBtn = gr.Button("Clear History")
+                max_length = gr.Slider(0, 32768, value=8192, step=1.0, label="Maximum length", interactive=True)
+                top_p = gr.Slider(0, 1, value=0.8, step=0.01, label="Top P", interactive=True)
+                temperature = gr.Slider(0, 1, value=0.95, step=0.01, label="Temperature", interactive=True)
+
+        history = gr.State([])
+        past_key_values = gr.State(None)
+
+        submitBtn.click(predict, [user_input, chatbot, max_length, top_p, temperature, history, past_key_values],
+                        [chatbot, history, past_key_values], show_progress=True)
+        submitBtn.click(reset_user_input, [], [user_input])
+
+        emptyBtn.click(reset_state, outputs=[chatbot, history, past_key_values], show_progress=True)
+
+    demo.queue().launch(server_name='0.0.0.0', share=False, inbrowser=True)
+else:
+    torch.npu.set_device(args.device + torch.distributed.get_rank())
+    history, past_key_values = [], None
+    while True:
+        object_list = [None] * 4
+        torch.distributed.broadcast_object_list(object_list, src=0)
+        query, max_length, top_p, temperature = object_list
+        for response, history, past_key_values in model.stream_chat(tokenizer, query, history,
+                                                                    past_key_values=past_key_values, return_past_key_values=True,
+                                                                    max_length=max_length, top_p=top_p, temperature=temperature):
+            # 因为 rank 0 和 rank 1 推理出的 response 是一致的
+            # 所以只需取 rank 0 推理出的 response 显示在网页上即可，rank 1 的推理结果不需要做任何操作
+            # 但仍然需要 for 循环调用 stream_chat，以便 rank 0 在 all_reduce 时能接收到 rank 1 的数据
+            pass
-- 
