{
    "NETWORK_SIZE": [
        "--num-layers", "32",
        "--hidden-size", "4096",
        "--ffn-hidden-size", "11008",
        "--num-attention-heads", "32",
        "--max-position-embeddings", "32768",
        "--position-embedding-type", "rope",
        "--make-vocab-size-divisible-by", "16",
        "--normalization", "RMSNorm",
        "--swiglu",
        "--untie-embeddings-and-output-weights",
        "--add-qkv-bias"
    ],

    "TOKENIZER_PARAM": [
        "--tokenizer-type", "PretrainedFromHF",
        "--tokenizer-name-or-path", "/home/dataset/qwen-7b-hf/"
    ],

    "DISTRIBUTED_PARAM": [
        "--tensor-model-parallel-size", "1",
        "--pipeline-model-parallel-size", "1"
    ],

    "AUXILIARY_PARAM": [
        "--micro-batch-size", "2",
        "--global-batch-size", "64",
        "--no-masked-softmax-fusion",
        "--disable-bias-linear",
        "--no-gradient-accumulation-fusion",
        "--bf16",
        "--seed", "42",
        "--use-fused-rmsnorm",
        "--no-load-optim",
        "--no-load-rng",
        "--seq-length", "8192",
        "--padded-vocab-size", "151936",
        "--attention-softmax-in-fp32"
    ],

    "OUTPUT_PARAM": [
        "--log-interval", "1",
        "--save-interval", "10000",
        "--eval-interval", "1000",
        "--eval-iters", "5"
    ],

    "INSTRUCTION_PARAM": [
        "--finetune",
        "--is-instruction-dataset",
        "--data-path", "/home/dataset/tune-dataset-qwen-7B/alpaca",
        "--split", "90,5,5",
        "--train-iters", "5"
    ],

    "DISTRIBUTED_PARAM_TP8_PP1": [
        "--tensor-model-parallel-size", "8",
        "--pipeline-model-parallel-size", "1"
    ],

    "PROCESS_INSTRUCTION_DATA": [
        "--input", "train-00000-of-00001-a09b74b3ef9c3b56, alpaca_zh, sharegpt1, sharegpt2",
        "--tokenizer-type", "PretrainedFromHF",
        "--handler-name", "LlamaFactoryInstructionHandler",
        "--output-prefix", "/home/dataset/tune-dataset-qwen-7B/lfhandler_tune_dataset/alpaca",
        "--tokenizer-name-or-path", "/home/dataset/qwen-7b-hf/",
        "--workers", "4",
        "--log-interval", "1000",
        "--append-eod", 
        "--lla-fact-ins-template", "qwen", 
        "--dataset-dir", "/home/dataset/tune-dataset-qwen-7B/lfhandler_tune_dataset/dataset/",
        "--overwrite-cache"
    ],


    "PROCESS_INSTRUCTION_DATA_MIX1": [
        "--input", "train-00000-of-00001-a09b74b3ef9c3b56, alpaca_zh, sharegpt1, sharegpt2",
        "--tokenizer-type", "PretrainedFromHF",
        "--handler-name", "LlamaFactoryInstructionHandler",
        "--output-prefix", "/home/dataset/tune-dataset-qwen-7B/lfhandler_tune_dataset/alpaca",
        "--tokenizer-name-or-path", "/home/dataset/qwen-7b-hf/",
        "--workers", "4",
        "--log-interval", "1000",
        "--append-eod", 
        "--lla-fact-ins-template", "qwen", 
        "--dataset-dir", "/home/dataset/tune-dataset-qwen-7B/lfhandler_tune_dataset/dataset/",
        "--overwrite-cache",
        "--interleave-probs", "0.1, 0.2, 0.3, 0.4",
        "--mix-strategy", "interleave_under",
        "--max-samples", "10"
    ],

    "PROCESS_INSTRUCTION_DATA_MIX2": [
        "--input", "train-00000-of-00001-a09b74b3ef9c3b56, alpaca_zh, sharegpt1, sharegpt2",
        "--tokenizer-type", "PretrainedFromHF",
        "--handler-name", "LlamaFactoryInstructionHandler",
        "--output-prefix", "/home/dataset/tune-dataset-qwen-7B/lfhandler_tune_dataset/alpaca",
        "--tokenizer-name-or-path", "/home/dataset/qwen-7b-hf/",
        "--workers", "4",
        "--log-interval", "1000",
        "--append-eod", 
        "--lla-fact-ins-template", "qwen", 
        "--dataset-dir", "/home/dataset/tune-dataset-qwen-7B/lfhandler_tune_dataset/dataset/",
        "--overwrite-cache",
        "--interleave-probs", "0.1, 0.2, 0.3, 0.4",
        "--mix-strategy", "interleave_over",
        "--max-samples", "10"
    ],


    "INFERENCE_PARAM": [
        "--max-new-tokens", "256",
        "--tokenizer-not-use-fast",
        "--exit-on-missing-checkpoint",
        "--attention-softmax-in-fp32",
        "--lla-fact-ins-template", "qwen",
        "--seed", "42",
        "--load", "/home/dataset/Qwen-7B-v0.1-tp8-pp1/"
    ], 


    "BEAM_SEARCH_AUXILIARY_PARAM": [
        "--task", "beam_search",
        "--top-p", "0.95",
        "--top-k", "50"
    ],

    "GREEDY_SEARCH_AUXILIARY_PARAM": [
        "--task", "greedy"
    ],

    "DO_SAMPLE_AUXILIARY_PARAM": [
        "--task", "do_sample",
        "--top-p", "0.95",
        "--top-k", "50"
    ],

    "BEAM_SEARCH_WITH_SAMPLING_AUXILIARY_PARAM": [
        "--task", "beam_search_with_sampling",
        "--top-p", "0.95",
        "--top-k", "50"
    ],

    "RETURN_OUTPUT_LOG_PROBS_AUXILIARY_PARAM": [
        "--task", "return_output_log_probs",
        "--temperature 0.6",
        "--top-p", "0.95",
        "--top-k", "50"
    ],

    "EVALUATION_PARAM_MMLU": [
        "--tokenizer-not-use-fast",
        "--task-data-path", "/home/dataset/eval_dataset/mmlu/test/",
        "--task", "mmlu",
        "--max-new-tokens", "1",
        "--exit-on-missing-checkpoint",
        "--max-eval-samples", "2",
        "--lla-fact-ins-template", "qwen"
    ],

    "EVALUATION_PARAM_BOOLQ": [
        "--tokenizer-not-use-fast",
        "--task-data-path", "/home/dataset/eval_dataset/boolq/test/",
        "--task", "boolq",
        "--max-new-tokens", "1",
        "--exit-on-missing-checkpoint",
        "--max-eval-samples", "30",
        "--lla-fact-ins-template", "qwen"
    ],

    "EVALUATION_PARAM_CEVAL": [
        "--tokenizer-not-use-fast",
        "--task-data-path", "/home/dataset/eval_dataset/ceval/val/",
        "--task", "ceval",
        "--max-new-tokens", "1",
        "--exit-on-missing-checkpoint",
        "--max-eval-samples", "2",
        "--lla-fact-ins-template", "qwen"
    ]
}