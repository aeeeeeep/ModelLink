{
    "test_evaluate": [
        {
            "param": {
                "task-data-path":"tests/test_data/datasets/evaluate_data/mmlu",
                "task":"mmlu",
                "tensor-model-parallel-size": 4,  
                "pipeline-model-parallel-size": 2,  
                "num-layers": 2,
                "num-attention-heads": 8, 
                "hidden-size": 1024, 
                "ffn-hidden-size": 1024,  
                "seq-length": 1024, 
                "max-new-tokens": 1, 
                "max-position-embeddings": 1024,   
                "group-query-attention": null, 
                "num-query-groups": 4, 
                "padded-vocab-size":65024, 
                "make-vocab-size-divisible-by": 1, 
                "micro-batch-size": 1, 
                "normalization": "RMSNorm", 
                "position-embedding-type": "rope", 
                "use-partial-rope":null, 
                "load":"tests/test_data/datasets/ckpts/chatglm-6b-tp4pp2",
                "tokenizer-type":"PretrainedFromHF",  
                "tokenizer-name-or-path":"tests/test_data/tokenizers/chatglm_6b_hf_tokenizer/", 
                "tokenizer-not-use-fast":null,  
                "untie-embeddings-and-output-weights":null ,
                "disable-bias-linear":null, 
                "add-qkv-bias":null, 
                "swiglu":null, 
                "fp16":null,  
                "exit-on-missing-checkpoint":null, 
                "no-load-rng":null, 
                "no-load-optim":null
            }
        },
        {
            "param": {
                "task-data-path":"tests/test_data/datasets/evaluate_data/mmlu",
                "task":"mmlu",
                "tensor-model-parallel-size": 2,  
                "pipeline-model-parallel-size": 4,  
                "num-layers": 4,
                "num-attention-heads": 8, 
                "hidden-size": 1024,  
                "ffn-hidden-size": 1024, 
                "seq-length": 1024, 
                "max-new-tokens": 2, 
                "max-position-embeddings": 1024,   
                "make-vocab-size-divisible-by": 32, 
                "micro-batch-size": 1, 
                "normalization": "RMSNorm", 
                "position-embedding-type": "rope", 
                "load":"tests/test_data/datasets/ckpts/internlm-7b-tp2pp4",
                "tokenizer-type":"PretrainedFromHF",  
                "tokenizer-name-or-path":"tests/test_data/tokenizers/internlm_hf_tokenizer/",  
                "tokenizer-not-use-fast":null,  
                "untie-embeddings-and-output-weights":null ,
                "add-qkv-bias":null, 
                "add-dense-bias":null,
                "skip-bias-add": null,
                "disable-bias-linear":null,
                "swiglu":null, 
                "fp16":null,  
                "exit-on-missing-checkpoint":null, 
                "no-load-rng":null, 
                "no-load-optim":null, 
                "no-masked-softmax-fusion":null
            }
        },
        {
            "param": {
                "task-data-path":"tests/test_data/datasets/evaluate_data/mmlu",
                "task":"mmlu",
                "tensor-model-parallel-size": 8,  
                "pipeline-model-parallel-size": 1,  
                "sequence-parallel":null,
                "num-layers": 2,
                "num-attention-heads": 8, 
                "hidden-size": 1024,  
                "ffn-hidden-size": 1024, 
                "seq-length": 1024, 
                "max-new-tokens": 1, 
                "max-position-embeddings": 1024,
                "group-query-attention": null, 
                "num-query-groups": 8, 
                "make-vocab-size-divisible-by": 1, 
                "micro-batch-size": 1, 
                "normalization": "RMSNorm", 
                "position-embedding-type": "rope", 
                "load":"tests/test_data/datasets/ckpts/mixtral-8x7b-tp8pp1ep1",
                "tokenizer-type":"PretrainedFromHF",   
                "tokenizer-name-or-path":"tests/test_data/tokenizers/mixtral_hf_tokenizer/", 
                "untie-embeddings-and-output-weights":null ,
                "disable-bias-linear":null,
                "use-fused-rmsnorm": null, 
                "swiglu":null,
                "no-masked-softmax-fusion":null,
                "attention-softmax-in-fp32":null,  
                "bf16":null,  
                "no-load-rng":null, 
                "no-load-optim":null,
                "num-experts": 8,
                "moe-router-topk": 2,
                "moe-train-capacity-factor": 8.0
            }
        },
        {
            "param": {
                "tensor-model-parallel-size": 8,
                "pipeline-model-parallel-size": 1,
                "micro-batch-size": 4,
                "tokenizer-type": "PretrainedFromHF",
                "tokenizer-name-or-path": "tests/test_data/tokenizers/Bloom-7B",
                "num-layers": 2,
                "hidden-size": 1024,
                "num-attention-heads": 8,
                "seq-length": 1024,
                "bf16": true,
                "max-position-embeddings": 1024,
                "max-new-tokens": 256,
                "load": "tests/test_data/ckpts/Bloom-7B",
                "task": "mmlu",
                "task-data-path": "tests/test_data/datasets/evaluate_data/mmlu",
                "no-load-optim": true,
                "no-load-rng": true,
                "exit-on-missing-checkpoint": true,
                "sequence-parallel": true,
                "padded-vocab-size": 250880,
                "embed-layernorm": true,
                "global-batch-size": 1,
                "make-vocab-size-divisible-by": 1,
                "attention-dropout": 0.0,
                "init-method-std": 0.01,
                "hidden-dropout": 0.0,
                "position-embedding-type": "alibi",
                "normalization": "LayerNorm",
                "no-masked-softmax-fusion": true,
                "attention-softmax-in-fp32": true,
                "weight-decay": 1e-1,
                "lr-warmup-fraction": 0.01,
                "clip-grad": 1.0,
                "adam-beta1": 0.9,
                "initial-loss-scale": 65536,
                "adam-beta2": 0.95,
                "no-gradient-accumulation-fusion": true
            }
        },
        {
            "param": {
                "task-data-path": "tests/test_data/datasets/evaluate_data/mmlu",
                "task": "mmlu",
                "seq-length": 32768,
                "max-new-tokens": 1,
                "max-position-embeddings": 32768,
                "tensor-model-parallel-size": 4,
                "pipeline-model-parallel-size": 2,
                "num-layers": 2,
                "hidden-size": 1024 ,
                "ffn-hidden-size": 1024,
                "num-attention-heads": 8 ,
                "disable-bias-linear": null,
                "swiglu": null,
                "position-embedding-type": "rope",
                "load": "tests/test_data/ckpts/Qwen-72B",
                "normalization": "RMSNorm",
                "tokenizer-type": "PretrainedFromHF" ,
                "tokenizer-name-or-path": "tests/test_data/tokenizers/Qwen-72B",
                "tokenizer-not-use-fast": null,
                "bf16": null,
                "micro-batch-size": 1,
                "exit-on-missing-checkpoint": null,
                "no-load-rng": null,
                "no-load-optim": null,
                "untie-embeddings-and-output-weights": null,
                "add-qkv-bias": null,
                "tokenizer-kwargs": ["eos_token", "<|endoftext|>", "pad_token", "<|extra_0|>"],
                "make-vocab-size-divisible-by": 8,
                "seed": 42
            }
        }
    ]
}

