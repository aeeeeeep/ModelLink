{
    "test_pretrain": [
        {
            "llama2_param": {
                "tensor-model-parallel-size": 2,
                "pipeline-model-parallel-size": 2,
                "micro-batch-size": 1,
                "tokenizer-type": "Llama2Tokenizer",
                "tokenizer-model": "tests/test_data/tokenizers/llama2_hf_tokenizer/tokenizer.model",
                "num-layers": 2,
                "hidden-size": 1024,
                "num-attention-heads": 8,
                "seq-length": 1024,
                "bf16": null,
                "split": "100,0,0",
                "max-position-embeddings": 1024,
                "lr": 1.25e-6,
                "train-iters": 3,
                "save-interval": 10000,
                "data-path": "tests/test_data/datasets/llama_mmap/pretrain_mmap_dataset/pretrain_text_document"
            }
        },
        {
            "Bloom7B_param": {
                "tensor-model-parallel-size": 8,
                "pipeline-model-parallel-size": 1,
                "micro-batch-size": 1,
                "tokenizer-type": "PretrainedFromHF",
                "tokenizer-name-or-path": "tests/test_data/tokenizers/Bloom-7B",
                "num-layers": 2,
                "hidden-size": 1024,
                "num-attention-heads": 8,
                "seq-length": 1024,
                "bf16": null,
                "split": "100,0,0",
                "max-position-embeddings": 1024,
                "lr": 1.2e-4,
                "train-iters": 2,
                "save-interval": 10000,
                "data-path": "tests/test_data/datasets/Bloom-7B/alpaca_text_document",
                "sequence-parallel": null,
                "global-batch-size": 512,
                "embed-layernorm": null,
                "padded-vocab-size": 250880,
                "make-vocab-size-divisible-by": 1,
                "attention-softmax-in-fp32": null,
                "apply-query-key-layer-scaling": null,
                "init-method-std": 0.0048,
                "hidden-dropout": 0.0,
                "attention-dropout": 0.0,
                "position-embedding-type": "alibi",
                "normalization": "LayerNorm",
                "min-lr": 6e-6,
                "lr-decay-iters": 200,
                "weight-decay": 1e-1,
                "clip-grad": 1.0,
                "adam-beta1": 0.9,
                "initial-loss-scale": 4096,
                "adam-beta2": 0.95,
                "no-gradient-accumulation-fusion": null,
                "no-load-optim": null,
                "no-load-rng": null,
                "seed": 42
            }
        },
        {
            "Qwen72B_param": {
                "tensor-model-parallel-size": 4,
                "pipeline-model-parallel-size": 2,
                "sequence-parallel": null,
                "num-layers": 2,
                "hidden-size": 1024,
                "ffn-hidden-size": 1024,
                "num-attention-heads": 8,
                "tokenizer-type": "PretrainedFromHF",
                "tokenizer-name-or-path": "tests/test_data/tokenizers/Qwen-72B",
                "seq-length": 32768,
                "recompute-granularity": "full",
                "recompute-method": "block",
                "recompute-num-layers": 2,
                "max-position-embeddings": 32768,
                "micro-batch-size": 4,
                "global-batch-size": 8,
                "make-vocab-size-divisible-by": 8,
                "lr": 1e-5,
                "train-iters": 2, 
                "lr-decay-style": "cosine",
                "untie-embeddings-and-output-weights": null,
                "disable-bias-linear": null,
                "attention-dropout": 0.0,
                "init-method-std": 0.01,
                "hidden-dropout": 0.0,
                "position-embedding-type": "rope",
                "normalization": "RMSNorm",
                "use-fused-rmsnorm": null,
                "swiglu": null,
                "use-flash-attn": null,
                "no-masked-softmax-fusion": null,
                "attention-softmax-in-fp32": null,
                "min-lr": 1.0e-6,
                "weight-decay": 1e-2,
                "lr-warmup-fraction": 0.1,
                "clip-grad": 1.0,
                "adam-beta1": 0.9,
                "adam-beta2": 0.95,
                "add-qkv-bias": null,
                "rotary-base": 1000000,
                "initial-loss-scale": 4096,
                "no-gradient-accumulation-fusion": null,
                "no-load-optim": null,
                "no-load-rng": null,
                "bf16": null,
                "split": "100,0,0",
                "save-interval": 10000,
                "data-path": "tests/test_data/datasets/Q-7B/alpaca_text_document",
                "tokenizer-kwargs": ["eos_token", "<|endoftext|>", "pad_token", "<|extra_0|>"]
            }
        }
    ]
}