{
    "test_pretrain": [
        {
            "param": {
                "tensor-model-parallel-size":4,
                "pipeline-model-parallel-size":2, 
                "sequence-parallel":null,
                "num-layers":2, 
                "num-attention-heads":8, 
                "hidden-size":1024, 
                "ffn-hidden-size":1024, 
                "seq-length":1024, 
                "max-position-embeddings":1024, 
                "micro-batch-size":1, 
                "global-batch-size":1, 
                "make-vocab-size-divisible-by":1, 
                "train-iters":1, 
                "lr":1.25e-6, 
                "min-lr": 1.25e-7, 
                "lr-warmup-fraction":0.01, 
                "lr-decay-style":"cosine", 
                "weight-decay": 1e-1,
                "clip-grad":1.0, 
                "adam-beta1": 0.9, 
                "initial-loss-scale": 65536, 
                "adam-beta2": 0.95, 
                "attention-dropout": 0.0 ,
                "init-method-std": 0.01, 
                "hidden-dropout": 0.0, 
                "normalization": "RMSNorm",
                "position-embedding-type": "rope",  
                "load":"tests/test_data/datasets/ckpts/llama2-7b-tp4pp2",
                "lora-load":"tests/test_data/datasets/ckpts/llama-2-7b-lora-tp4pp2", 
                "data-path": "tests/test_data/datasets/llama_mmap/pretrain_mmap_dataset/instrustion_text_document",
                "split": "100,0,0",
                "tokenizer-type": "PretrainedFromHF", 
                "tokenizer-name-or-path":"tests/test_data/tokenizers/llama2_hf_tokenizer/", 
                "tokenizer-not-use-fast":null, 
                "untie-embeddings-and-output-weights":null, 
                "disable-bias-linear":null,
                "use-fused-rmsnorm": null, 
                "swiglu":null,
                "use-flash-attn":null, 
                "no-masked-softmax-fusion":null, 
                "attention-softmax-in-fp32":null,  
                "no-gradient-accumulation-fusion":null, 
                "no-load-optim":null, 
                "no-load-rng":null,
                "finetune":null, 
                "is-instruction-dataset":null, 
                "lora-r": 16, 
                "lora-alpha": 32, 
                "lora-target-modules": ["query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"], 
                "bf16":null,
                "log-interval": 1,
                "save-interval": 10000
            }
        },
        {
            "param": {
                "tensor-model-parallel-size":4,
                "pipeline-model-parallel-size":2, 
                "sequence-parallel":null,
                "num-layers":2, 
                "num-attention-heads":8, 
                "hidden-size":1024, 
                "ffn-hidden-size":1024, 
                "seq-length":1024, 
                "max-position-embeddings":1024, 
                "micro-batch-size":1, 
                "global-batch-size":1, 
                "make-vocab-size-divisible-by":1, 
                "train-iters":1, 
                "lr":1.25e-6, 
                "min-lr": 1.25e-7, 
                "lr-warmup-fraction":0.01, 
                "lr-decay-style":"cosine", 
                "weight-decay": 1e-1,
                "clip-grad":1.0, 
                "adam-beta1": 0.9, 
                "initial-loss-scale": 65536, 
                "adam-beta2": 0.95, 
                "attention-dropout": 0.0 ,
                "init-method-std": 0.01, 
                "hidden-dropout": 0.0, 
                "normalization": "RMSNorm",
                "position-embedding-type": "rope",  
                "load":"tests/test_data/datasets/ckpts/llama2-7b-tp4pp2",
                "data-path": "tests/test_data/datasets/llama_mmap/pretrain_mmap_dataset/pretrain_text_document",
                "split": "100,0,0",
                "tokenizer-type": "PretrainedFromHF", 
                "tokenizer-name-or-path":"tests/test_data/tokenizers/llama2_hf_tokenizer/", 
                "tokenizer-not-use-fast":null, 
                "untie-embeddings-and-output-weights":null, 
                "disable-bias-linear":null,
                "use-fused-rmsnorm": null, 
                "swiglu":null,
                "use-flash-attn":null, 
                "no-masked-softmax-fusion":null, 
                "attention-softmax-in-fp32":null,  
                "no-gradient-accumulation-fusion":null, 
                "no-load-optim":null, 
                "no-load-rng":null,
                "use-distributed-optimizer": null,
                "use-fused-swiglu": null,
                "use-fused-rotary-pos-emb": null,
                "overlap-grad-reduce": null,
                "finetune":null, 
                "is-instruction-dataset":null, 
                "bf16":null,
                "log-interval": 1,
                "save-interval": 10000
            }
        },
        {
            "param": {
                "tensor-model-parallel-size":4,
                "pipeline-model-parallel-size":2, 
                "sequence-parallel":null,
                "num-layers":2, 
                "num-attention-heads":8, 
                "hidden-size":1024, 
                "ffn-hidden-size":1024, 
                "seq-length":1024, 
                "max-position-embeddings":1024, 
                "padded-vocab-size": 65024,
                "micro-batch-size":1, 
                "global-batch-size":1, 
                "make-vocab-size-divisible-by":1, 
                "group-query-attention":null,
                "num-query-groups":4, 
                "train-iters":2, 
                "lr":1.25e-6, 
                "min-lr": 1e-8, 
                "lr-warmup-fraction":0.01, 
                "lr-decay-style":"cosine", 
                "weight-decay": 1e-1,
                "clip-grad":1.0, 
                "adam-beta1": 0.9, 
                "initial-loss-scale": 4096, 
                "adam-beta2": 0.95, 
                "attention-dropout": 0.0 ,
                "init-method-std": 0.01, 
                "hidden-dropout": 0.0, 
                "normalization": "RMSNorm",
                "position-embedding-type": "rope",  
                "load":"tests/test_data/datasets/ckpts/chatglm-6b-tp4pp2",
                "data-path": "tests/test_data/datasets/llama_mmap/pretrain_mmap_dataset/pretrain_text_document",
                "split": "100,0,0",
                "tokenizer-type": "PretrainedFromHF", 
                "tokenizer-name-or-path":"tests/test_data/tokenizers/chatglm_6b_hf_tokenizer/", 
                "untie-embeddings-and-output-weights":null, 
                "add-qkv-bias": null,
                "use-partial-rope": null,
                "disable-bias-linear":null,
                "use-fused-rmsnorm": null, 
                "swiglu":null,
                "use-flash-attn":null, 
                "use-distributed-optimizer": null,
                "no-masked-softmax-fusion":null, 
                "attention-softmax-in-fp32":null,  
                "no-gradient-accumulation-fusion":null, 
                "no-load-optim":null, 
                "no-load-rng":null,
                "fp16":null,
                "log-interval": 1,
                "save-interval": 10000
            }
        },
        {
            "param": {
                "tensor-model-parallel-size":2,
                "pipeline-model-parallel-size":4, 
                "sequence-parallel":null,
                "num-layers":4, 
                "num-attention-heads":8, 
                "hidden-size":1024, 
                "ffn-hidden-size":1024, 
                "seq-length":1024, 
                "max-position-embeddings":1024, 
                "micro-batch-size":1, 
                "global-batch-size":1, 
                "make-vocab-size-divisible-by":32, 
                "train-iters":2, 
                "lr":1.25e-6, 
                "min-lr": 1.25e-7, 
                "lr-warmup-fraction":0.01, 
                "lr-decay-style":"cosine", 
                "weight-decay": 1e-1,
                "clip-grad":1.0, 
                "adam-beta1": 0.9, 
                "initial-loss-scale": 65536, 
                "adam-beta2": 0.95, 
                "attention-dropout": 0.0 ,
                "init-method-std": 0.01, 
                "hidden-dropout": 0.0, 
                "normalization": "RMSNorm",
                "position-embedding-type": "rope",  
                "load":"tests/test_data/datasets/ckpts/internlm-7b-tp2pp4",
                "data-path": "tests/test_data/datasets/llama_mmap/pretrain_mmap_dataset/pretrain_text_document",
                "split": "100,0,0",
                "tokenizer-type": "Llama2Tokenizer", 
                "tokenizer-name-or-path":"tests/test_data/tokenizers/internlm_hf_tokenizer/", 
                "untie-embeddings-and-output-weights":null, 
                "add-qkv-bias": null,
                "add-dense-bias":null,
                "skip-bias-add": null,
                "disable-bias-linear":null,
                "use-fused-rmsnorm": null, 
                "use-flash-attn":null, 
                "use-fused-rotary-pos-emb": null,
                "use-rotary-position-embeddings": null,
                "swiglu":null,
                "no-masked-softmax-fusion":null, 
                "attention-softmax-in-fp32":null,  
                "no-gradient-accumulation-fusion":null, 
                "no-load-optim":null, 
                "no-load-rng":null,
                "bf16":null,
                "log-interval": 1,
                "save-interval": 10000
            }
        },
        {
            "param": {
                "tensor-model-parallel-size":8,
                "pipeline-model-parallel-size":1, 
                "sequence-parallel":null,
                "recompute-method": "block",
                "recompute-granularity":"full",
                "recompute-num-layers":2,
                "num-layers":2, 
                "num-attention-heads":8, 
                "hidden-size":1024, 
                "ffn-hidden-size":1024, 
                "seq-length":1024, 
                "max-position-embeddings":1024, 
                "micro-batch-size":1, 
                "global-batch-size":1, 
                "make-vocab-size-divisible-by":1, 
                "group-query-attention":null,
                "num-query-groups":8, 
                "train-iters":2, 
                "lr":1.25e-6, 
                "min-lr": 1.25e-7, 
                "lr-warmup-fraction":0.01, 
                "lr-decay-style":"cosine", 
                "weight-decay": 1e-1,
                "clip-grad":1.0, 
                "adam-beta1": 0.9, 
                "initial-loss-scale": 65536, 
                "adam-beta2": 0.95, 
                "attention-dropout": 0.0 ,
                "init-method-std": 0.01, 
                "hidden-dropout": 0.0, 
                "normalization": "RMSNorm",
                "position-embedding-type": "rope",  
                "load":"tests/test_data/datasets/ckpts/mixtral-8x7b-tp8pp1ep1",
                "data-path": "tests/test_data/datasets/llama_mmap/pretrain_mmap_dataset/pretrain_text_document",
                "split": "100,0,0",
                "tokenizer-type": "PretrainedFromHF", 
                "tokenizer-name-or-path":"tests/test_data/tokenizers/mixtral_hf_tokenizer/", 
                "untie-embeddings-and-output-weights":null, 
                "disable-bias-linear":null,
                "use-fused-rmsnorm": null, 
                "use-flash-attn":null, 
                "swiglu":null,
                "no-masked-softmax-fusion":null, 
                "attention-softmax-in-fp32":null,  
                "no-gradient-accumulation-fusion":null, 
                "no-load-optim":null, 
                "no-load-rng":null,
                "bf16":null,
                "num-experts": 8,
                "expert-model-parallel-size":1,
                "moe-router-topk": 2,
                "moe-router-load-balancing-type": "aux_loss",
                "moe-aux-loss-coeff": 0.01,
                "moe-train-capacity-factor": 1.1,
                "noisy-gate-policy": "RSample",
                "log-interval": 1,
                "save-interval": 10000
            }
        }
    ]
}